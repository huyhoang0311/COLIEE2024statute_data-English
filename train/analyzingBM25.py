import json
from rank_bm25 import BM25Okapi
from nltk.tokenize import word_tokenize
import nltk
import numpy as np
import csv
import pandas as pd
# T·∫£i b·ªô d·ªØ li·ªáu c·∫ßn thi·∫øt cho NLTK
nltk.download('punkt_tab')

# ƒê·ªçc n·ªôi dung t·ª´ file JSON
def load_json_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return json.load(file)

# Chuy·ªÉn d·ªØ li·ªáu th√†nh corpus (danh s√°ch c√°c vƒÉn b·∫£n)
def create_corpus(data):
    return [content.replace("\n", " ").strip() for content in data.values()]

# Token h√≥a vƒÉn b·∫£n
def tokenize_corpus(corpus):
    return [word_tokenize(doc.lower()) for doc in corpus]

# ƒê√°nh gi√° F2-score
def evaluate_F2_single(label_set, predict_set):
    correct_retrieved = len(label_set.intersection(predict_set))
    precision = correct_retrieved / len(predict_set) if len(predict_set) > 0 else 0
    recall = correct_retrieved / len(label_set) if len(label_set) > 0 else 0
    if precision + recall == 0:
        f2_measure = 0
    else:
        f2_measure = (5 * precision * recall) / (4 * precision + recall)
    return precision, recall, f2_measure

def evaluate_F2_overall(queries):
    total_precision = 0
    total_recall = 0
    num_queries = len(queries)
    for label_set, predict_set in queries:
        precision, recall, _ = evaluate_F2_single(label_set, predict_set)
        total_precision += precision
        total_recall += recall
    avg_precision = total_precision / num_queries if num_queries > 0 else 0
    avg_recall = total_recall / num_queries if num_queries > 0 else 0
    if avg_precision + avg_recall == 0:
        overall_f2 = 0
    else:
        overall_f2 = (5 * avg_precision * avg_recall) / (4 * avg_precision + avg_recall)
    return avg_precision, avg_recall, overall_f2

# ƒê√°nh gi√° Recall theo top_k
def evaluate_recall(corpus, articles, training_data, bm25, tokenized_corpus, scores_per_query, top_k):
    corpus_keys = list(articles.keys())
    total_queries = []
    rank_distribution = {
                "0-10": 0,
                "11-30": 0,
                "31-50": 0,
                "51-100": 0,
                "101-200": 0,
                "201-500": 0,
                "Out of Top 500": 0
    }
    for query, expected_keys in training_data.items():
        scores = scores_per_query[query]
        top_k_idx = np.argsort(scores)[::-1]  # L·∫•y top-k nhanh b·∫±ng numpy
        top_k_keys = [corpus_keys[idx] for idx in top_k_idx]

        for key in expected_keys:
            rank = top_k_keys.index(key) + 1  # X·∫øp h·∫°ng b·∫Øt ƒë·∫ßu t·ª´ 1
            # Ph√¢n lo·∫°i rank v√†o nh√≥m ph√π h·ª£p
            if rank <= 10:
                rank_distribution["0-10"] += 1
            elif rank <= 30:
                rank_distribution["11-30"] += 1
            elif rank <= 50:
                rank_distribution["31-50"] += 1
            elif rank <= 100:
                rank_distribution["51-100"] += 1
            elif rank <= 200:
                rank_distribution["101-200"] += 1
            elif rank <= 500:
                rank_distribution["201-500"] += 1
            else:
                rank_distribution["Out of Top 500"] += 1

            # In k·∫øt qu·∫£
    summ = sum(rank_distribution.values())
    print("üìä **Th·ªëng k√™ Expected Key theo v·ªã tr√≠ trong b·∫£ng x·∫øp h·∫°ng BM25:**")
    for key, count in rank_distribution.items():
        print(f"{key}: {count}:{count/summ}")

    label_set = set(expected_keys)
    total_queries.append((label_set, top_k_keys))

    overall_precision, overall_recall, overall_f2 = evaluate_F2_overall(total_queries)
    return overall_precision,overall_recall,overall_f2




# ƒê·ªçc d·ªØ li·ªáu
#articles_path = "/kaggle/input/coliee/COLIEE2024statute_data-English/text/articlesFull.json"
#training_data_path = "/kaggle/input/coliee/COLIEE2024statute_data-English/text/TrainingData(2).json"

articles_path = "text/articlesFull.json"
training_data_path = "text/TrainingData(2).json"



articles = load_json_file(articles_path)
training_data = load_json_file(training_data_path)

if isinstance(articles, dict):
    corpus = create_corpus(articles)
else:
    print("D·ªØ li·ªáu articles.json kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng.")
    corpus = []

# T√≠nh ƒë·ªô ch√≠nh x√°c
if corpus and isinstance(training_data, dict):
    tokenized_corpus = tokenize_corpus(corpus)

    #ƒëi·ªÅu ch·ªânh tham s·ªë bm25
    #k1 = 1.55
    #b = 0.8
    bm25 = BM25Okapi(tokenized_corpus)

    # T√≠nh tr∆∞·ªõc ƒëi·ªÉm s·ªë cho m·ªói query ƒë·ªÉ kh√¥ng ph·∫£i ch·∫°y l·∫°i nhi·ªÅu l·∫ßn
    scores_per_query = {
        query: bm25.get_scores(word_tokenize(query.lower()))
        for query in training_data.keys()
    }
    top_ = []
    recall_ = []

    # for top_k in range(1, 501):
    #     recall = evaluate_recall(corpus, articles, training_data, bm25, tokenized_corpus, scores_per_query, top_k)
    #     print(f"Recall (Top-{top_k}): {recall * 100:.2f}%")
    #     recall = round(recall,4)
    #     top_.append(top_k)
    #     recall_.append(recall)
    #df = pd.DataFrame({"Top_k":top_,"Recall point":recall_})
    #df.to_csv("BM25_recall_result",index = False)   

    # top_k_list = [50]
    # for top_k in top_k_list :
    #     precision,recall,f2 = evaluate_recall(corpus, articles, training_data, bm25, tokenized_corpus, scores_per_query, top_k)
    #     print(f"Recall (Top-{top_k}): {recall * 100:.2f}%")
    #     print(precision)
    #     print(f2)
    #     recall = round(recall,4)
    #     top_.append(top_k)
    #     recall_.append(recall)

    # df = pd.DataFrame({"Top_k":top_,"Recall point":recall_})
    # df.to_csv("BM25_recall_result_with_BM25_shortlist", index=False)
else:
    print("D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá.")


import json
import numpy as np
import pandas as pd
from collections import defaultdict, Counter
from nltk.tokenize import word_tokenize

# Load d·ªØ li·ªáu mapping article -> chapter
true_chapters_path = "train/true_chapters.json"
with open(true_chapters_path, "r", encoding="utf-8") as file:
    article_to_chapter = json.load(file)

# X√¢y d·ª±ng √°nh x·∫° chapter -> danh s√°ch c√°c articles
chapter_to_articles = defaultdict(list)
for article, chapter in article_to_chapter.items():
    chapter_to_articles[chapter].append(article)

def evaluate_chapter_coverage(articles, training_data, bm25, tokenized_corpus, scores_per_query):
    corpus_keys = list(articles.keys())  # Danh s√°ch article IDs
    total_queries = 0
    matched_queries = 0
    article_counts = []

    for query, expected_keys in training_data.items():
        # T√≠nh ƒëi·ªÉm BM25 cho truy v·∫•n
        scores = scores_per_query[query]
        top_idx = np.argsort(scores)[-500:][::-1]  # Ch·ªçn 50 ƒëi·ªÅu lu·∫≠t c√≥ ƒëi·ªÉm cao nh·∫•t
        top_articles = [corpus_keys[idx] for idx in top_idx]

        # **ƒê·∫øm t·∫ßn su·∫•t xu·∫•t hi·ªán c·ªßa chapters trong top 50 articles**
        chapter_counts = Counter(article_to_chapter.get(article, -1) for article in top_articles)
        
        # **L·∫•y 3 chapter c√≥ s·ªë l∆∞·ª£ng articles nhi·ªÅu nh·∫•t**
        top_3_chapters = [chapter for chapter, _ in chapter_counts.most_common(22)]

        # **L·ªçc l·∫°i danh s√°ch articles trong 3 chapters ƒë∆∞·ª£c ch·ªçn**
        selected_articles = []
        for ch in top_3_chapters:
            if ch in chapter_to_articles:
                articles_in_chapter = chapter_to_articles[ch]  # Danh s√°ch articles c·ªßa ch∆∞∆°ng
                if len(articles_in_chapter) > 0:
                    # N·∫øu ch∆∞∆°ng c√≥ ‚â• 50 articles, l·∫•y 50 article c√≥ ƒëi·ªÉm BM25 cao nh·∫•t
                    article_scores = {art: scores[corpus_keys.index(art)] for art in articles_in_chapter if art in corpus_keys}
                    top_50_articles = sorted(article_scores, key=article_scores.get, reverse=True)
                    selected_articles.extend(top_50_articles)
                else:
                    # N·∫øu ch∆∞∆°ng c√≥ < 50 articles, l·∫•y to√†n b·ªô
                    selected_articles.extend(articles_in_chapter)

        article_counts.append(len(selected_articles))  # Ghi l·∫°i s·ªë l∆∞·ª£ng articles th·ª±c s·ª± ph·∫£i t√≠nh to√°n

        # **Ki·ªÉm tra xem expected_keys c√≥ n·∫±m trong danh s√°ch articles ƒë∆∞·ª£c ch·ªçn kh√¥ng**
        for expected_key in expected_keys:
            if expected_key in selected_articles:
                matched_queries += 1  # N·∫øu c√≥ √≠t nh·∫•t 1 expected key n·∫±m trong danh s√°ch l·ªçc

        total_queries += len(expected_keys)

    # **T√≠nh t·ª∑ l·ªá expected_keys n·∫±m trong t·∫≠p articles sau khi l·ªçc**
    match_ratio = matched_queries / total_queries if total_queries > 0 else 0
    print(f"T·ª∑ l·ªá expected_keys n·∫±m trong 3 ch∆∞∆°ng c√≥ s·ªë articles nhi·ªÅu nh·∫•t: {match_ratio:.2%}")

    # **T√≠nh to√°n th·ªëng k√™**
    mean_articles = np.mean(article_counts)
    median_articles = np.median(article_counts)
    max_articles = np.max(article_counts)
    min_articles = np.min(article_counts)

    print(f"üìä Th·ªëng k√™ s·ªë articles th·ª±c t·∫ø c·∫ßn t√≠nh to√°n:")
    print(f"Mean (trung b√¨nh): {mean_articles:.2f}")
    print(f"Median (trung v·ªã): {median_articles}")
    print(f"Max (l·ªõn nh·∫•t): {max_articles}")
    print(f"Min (nh·ªè nh·∫•t): {min_articles}")

    return match_ratio, mean_articles, median_articles, max_articles, min_articles

# Ch·∫°y ph√¢n t√≠ch
# match_ratio, mean_articles, median_articles, max_articles, min_articles = evaluate_chapter_coverage(
#     articles, training_data, bm25, tokenized_corpus, scores_per_query
# )


import json
import numpy as np
import pandas as pd
from collections import defaultdict



def check_chapter_in_top_k(articles, training_data, bm25, tokenized_corpus, scores_per_query, top_k_values=[10, 20, 50, 100]):
    corpus_keys = list(articles.keys())  # Danh s√°ch article IDs
    results = {k: {"total_queries": 0, "matched_queries": 0} for k in top_k_values}

    for query, expected_keys in training_data.items():
        # T√≠nh ƒëi·ªÉm BM25 cho truy v·∫•n
        scores = scores_per_query[query]
        sorted_indices = np.argsort(scores)[::-1]  # S·∫Øp x·∫øp theo ƒëi·ªÉm BM25 gi·∫£m d·∫ßn
        
        # L·∫•y ch∆∞∆°ng c·ªßa expected keys
        expected_chapters = {article_to_chapter.get(key, -1) for key in expected_keys}

        for top_k in top_k_values:
            top_k_articles = [corpus_keys[idx] for idx in sorted_indices[:top_k]]
            top_k_chapters = {article_to_chapter.get(article, -1) for article in top_k_articles}

            # Ki·ªÉm tra xem c√≥ expected chapter n√†o n·∫±m trong t·∫≠p chapter c·ªßa top-k articles kh√¥ng
            if any(ch in top_k_chapters for ch in expected_chapters):
                results[top_k]["matched_queries"] += 1

            results[top_k]["total_queries"] += 1

    # T√≠nh t·ª∑ l·ªá
    for top_k in top_k_values:
        total = results[top_k]["total_queries"]
        matched = results[top_k]["matched_queries"]
        match_ratio = matched / total if total > 0 else 0
        print(f"T·ª∑ l·ªá expected keys c√≥ chapter xu·∫•t hi·ªán trong top-{top_k} articles: {match_ratio:.2%}")

    return results

# Ch·∫°y ki·ªÉm tra
results = check_chapter_in_top_k(articles, training_data, bm25, tokenized_corpus, scores_per_query)
